{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c44bed1a-8525-4849-8d7c-6f5e4be560b1",
   "metadata": {},
   "source": [
    "## Digit Classification with a Single-Layer Perceptron\n",
    "Construction of a single layer perceptron from scratch and application to the binary classification of digits 0 and 1.\n",
    "\n",
    "This project was inspired by this [article](https://towardsdatascience.com/digit-classification-with-single-layer-perceptron-9a4e7d4d9628) written by Javier Martinez Ojeda, featured on Towards Data Science."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe5d861-fadd-4989-b391-f25dbccc44e5",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8d412a5-3d74-4b07-b649-e53fb8b4e533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43c27f7-3760-4b81-8892-fdaa1c101c13",
   "metadata": {},
   "source": [
    "### Perceptron components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9c92ac-5d2a-4925-b172-cbaa1bfdf2c1",
   "metadata": {},
   "source": [
    "#### Activation Function\n",
    "The given function implements the step activation function for binary classification in a perceptron. It takes a data point (x) as input along with the weights and bias terms. The function calculates a prediction by computing the dot product of the weights and the input, adding the bias, and then applying the step function. If the resulting prediction is greater than or equal to zero, it assigns a class label of 1; otherwise, it assigns a class label of 0. \n",
    "\n",
    "This step function introduces a decision boundary, allowing the perceptron to classify data points into two distinct classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0b02815-6788-428d-8e38-1ac8677b727e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the step activation function to predict the class of a given data point (x)\n",
    "def step_activation_function(x, weights, bias):\n",
    "    prediction = np.dot(weights, x) + bias\n",
    "    return 1 if prediction >= 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a997a9de-8a37-4aa8-a013-c89fcf858d0f",
   "metadata": {},
   "source": [
    "#### Forward Propagation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "facc4905-4fb8-4b6e-b3f4-aaf61ad86b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(x, y, weights, bias): \n",
    "    \"\"\"\n",
    "    x: training data as a vector (nparray), where each value corresponds\n",
    "        to a feature's value\n",
    "    y: label (0 or 1)\n",
    "    weights: weights of the perceptron\n",
    "    bias: bias\n",
    "    \"\"\"\n",
    "    y_pred = predict(x, weights, bias)\n",
    "    loss = (y_pred - y)**2   \n",
    "    d_loss = 2*(y_pred - y)\n",
    "    \n",
    "    return y_pred, loss, d_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac04bae-15e7-470a-a948-dbd7a43e99c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
